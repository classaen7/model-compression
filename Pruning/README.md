# Pruning
Pruningì€ ì¤‘ìš”ë„ê°€ ë‚®ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì´ë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ê°€ ì¡´ì¬í•œë‹¤.
- ë‹¨ìœ„: Structured(group) / Unstructured(fine grained)
- ì •ëŸ‰ì  ê¸°ì¤€: Magnitude(L2, L1), BN Scailing factor, Energy-based, ...
- ì ìš© ê¸°ì¤€: Network ì „ì²´ë¥¼ ê¸°ì¤€(global), Layer ë§ˆë‹¤ ë‚´ë¶€ ê¸°ì¤€(local)
- ì‹œì : í•™ìŠµëœ ëª¨ë¸ / ì´ˆê¸°í™” ì‹œì 

<p align="center">
  <img src="">
</p>

# Structured Pruning
Structured Pruningì€ íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ë£¹(channel, filter, layer) ë‹¨ìœ„ë¡œ ì œê±°í•˜ëŠ” ê²ƒì´ë‹¤.
<p align="center">
  <img src="https://github.com/user-attachments/assets/2ca5bf2c-2323-4dec-8e7f-6506794a563c">
</p>

### Learning Efficient Convolutional Networks through Network Slimming [CV]
> #Structured(group) #BN scailing factor #Global #Trained Model

Weightì˜ BNì˜ scailing factor $\gamma$ë¡œ ì¤‘ìš”ë„ë¥¼ íŒë‹¨í•œë‹¤.


### HRank: Filter Pruning using High-Rank Feature Map [CV]
> #Structured(group) #Feature mapì˜ Rank(SVD) #Local #Trained Model [CV]

Weightê°€ ì•„ë‹Œ Feture map outputì˜ SVD Rankë¡œ ì¤‘ìš”ë„ë¥¼ íŒë‹¨í•œë‹¤. 

<p align="center">
  <img src="https://github.com/user-attachments/assets/2747f544-e58e-4b9d-9d82-4204251995e1">
</p>

ìœ„ì™€ ê°™ì´ ë‹¤ì–‘í•œ ëª¨ë¸, ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ feature map outputì˜ SVD Rankì˜ ì°¨ì´ê°€ ì—†ìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•˜ì˜€ë‹¤.

### Are Sixteen Heads Really Better than One? [NLP]
Mutli-Headed Attention(MHA)ì€ ë³µí•©ì ì¸ ì˜ë¯¸ë“¤ì„ ë‹´ì•„ë‚´ê¸° ìœ„í•´ ë§ì€ ìˆ˜ì˜ HEADë¡œ êµ¬ì„±ë˜ì–´ìˆì§€ë§Œ ëª‡ê°œê°€ ì œê±°ë˜ì–´ë„ ì„±ëŠ¥ì˜ ê°ì†Œê°€ ì—†ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•œ ë…¼ë¬¸.

ë˜í•œ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜í•´ë„ HEADì˜ ì¤‘ìš”ë„ëŠ” ë¹„ìŠ·í•˜ë‹¤ëŠ” ì‹¤í—˜ ê²°ê³¼ê°€ ìˆë‹¤.

- **Iterative Pruning of Attention Heads**<br>
ì—¬ëŸ¬ layerì˜ ì—¬ëŸ¬ Headê°€ ì¡´ì¬í•  ë•Œ, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë“  Headë“¤ì˜ **proxy importance score**ë¥¼ êµ¬í•˜ê³  ì •ë ¬í•œ í›„ì— p%ì˜ HEADë¥¼ ì œê±°í•˜ëŠ” í˜•íƒœë¡œ Pruningì„ ìˆ˜í–‰í•œë‹¤.<br>
ì´ëŠ” íŠ¹ì • Headê°€ ì¡´ì¬í• ë•Œì™€ ê·¸ë ‡ì§€ ì•Šì„ ë•Œì˜ Loss ì°¨ì´ì˜ ì ˆëŒ€ê°’ì„, ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ê³„ì‚°ì„ í•œ í›„ í‰ê· ê°’ì„ êµ¬í•œ ê²ƒì´ë‹¤. 

# UnStructured Pruning
UnStructured Pruningì€ íŒŒë¼ë¯¸í„°ë¥¼ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ì˜ í–‰ë ¬ì€ í¬ì†Œ(Sparse)í•´ì§„ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤.
<font color="gray">í•˜ì§€ë§Œ Sparse Computationì— ìµœì í™”ëœ ì†Œí”„íŠ¸ì›¨ì–´ ë˜ëŠ” í•˜ë“œì›¨ì–´ê°€ ì•„ë‹Œ ì´ìƒ ì†ë„ í–¥ìƒì´ ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.</font>

### The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks [CV]
> #UnStructured(fine grained) #L1 norm #Global #Trained Model 

í° ì‹ ê²½ë§ì—ëŠ” ë³µê¶Œ í‹°ì¼“(Lottery Ticker)ê³¼ ê°™ì€ ì‘ì€ ì„œë¸Œë„¤íŠ¸ì›Œí¬ê°€ ì¡´ì¬í•˜ë©°, ì´ **ì„œë¸Œë„¤íŠ¸ì›Œí¬**ê°€ ì´ˆê¸°í™”ëœ ìƒíƒœë¡œ í•™ìŠµí•  ë•Œ ì›ë˜ì˜ í° ë„¤íŠ¸ì›Œí¬ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì£¼ì¥ì´ë‹¤.

í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒì˜ Lottery Ticketì„ ì°¾ëŠ” ë°©ë²•ì„ í†µí•´ 10% ~ 20%ì˜ weightë§Œìœ¼ë¡œ ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ì™€ ë™ì¼í•œ ì„±ëŠ¥ì„ ê°–ëŠ” ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ëŠ”ë‹¤. (ì•„ì£¼ ì•½ê°„ Pruneí•˜ê³  ì¬í•™ìŠµì„ ë°˜ë³µí•˜ëŠ” Iterative Pruningì„ ìˆ˜í–‰)

1. ë„¤íŠ¸ì›Œí¬ ì„ì˜ ì´ˆê¸°í™” $f(x;\theta _0)$
2. ë„¤íŠ¸ì›Œí¬ë¥¼ $j$ë²ˆ í•™ìŠµí•˜ì—¬ íŒŒë¼ë¯¸í„° $\theta _j$ë¥¼ ë„ì¶œ
3. íŒŒë¼ë¯¸í„° $\theta _j$ë¥¼ $p%$(ë³´í†µ 20%)ë§Œí¼ pruningí•˜ì—¬ mask **m**ì„ ìƒì„± 
4. Mask ë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ë¥¼ $\theta _0$ë¡œ ë˜ëŒë¦¬ê³  ì´ë¥¼ `winning ticket`ì´ë¼ ì§€ì¹­ ($f(x;m \odot \theta)$)
5. Target sparsityì— ë„ë‹¬í• ë•Œ ê¹Œì§€ 2-4ë¥¼ ë°˜ë³µ $(1-p%)^n$ = ë‚¨ì€ íŒŒë¼ë¯¸í„° ìˆ˜ %

ì‹œê°„ì€ ì˜¤ë˜ê±¸ë¦¬ì§€ë§Œ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ ë…¼ë¬¸ì´ë‹¤.

### Stabilizing the Lottery Ticket Hypothesis; Weight Rewinding [CV]
ìœ„ì˜ LTHì˜ ê²½ìš° ë°ì´í„°ì…‹ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ í¬ê¸°ê°€ ì»¤ì¡Œì„ ë•Œ ë¶ˆì•ˆì •í•œ ëª¨ìŠµì„ ë³´ì´ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.<br>
ì´ ë°©ë²•ì€ LTHì™€ ë‹¤ë¥´ê²Œ weightì™€ lr_schedulerë¥¼ ì²˜ìŒìœ¼ë¡œ ì´ˆê¸°í™” í•˜ëŠ”ê²Œ ì•„ë‹Œ kë²ˆì§¸ epochìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ í•™ìŠµì´ ì•ˆì •í™”ë˜ëŠ” ê²ƒì„ ë³´ì¸ ë…¼ë¬¸ì´ë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” ì´ iterationì˜ 0.1% ~ 7% ì •ë„ì˜ ì§€ì ì„ rewinding pointë¡œ ì–¸ê¸‰í•œë‹¤.

### Comparing Rewinding And Find-tuning In Neural Network Pruning; Learning Rate Rewinding [CV]
ìœ„ì˜ ë°©ë²•ì—ì„œ WeightëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  LR_Schedulerë§Œ íŠ¹ì • ì‹œì (k)ë¡œ rewinding í•˜ëŠ” ì „ëµì´ë‹¤.

### Linear Mode Connectivity and the Lottery Ticket Hypothesis [CV]
íŠ¹ì • initializationì— ëŒ€í•´ì„œ, í•™ìŠµ í›„ì— ë„ë‹¬í•˜ëŠ”(**ìˆ˜ë ´**ë˜ëŠ”) ê³µê°„ì´ ìœ ì‚¬í•œì§€ ì‹¤í—˜í•œ ë…¼ë¬¸ì´ë‹¤. ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. <br>
íŠ¹ì • ì‹œì (0, k)ì—ì„œ Seedë¥¼ ë³€ê²½í•˜ì—¬ ë‘ ê°œì˜ Netì„ í•™ìŠµì‹œí‚¨ë‹¤. ì´ë•Œ ë‘˜ê°„ì˜ Weightë¥¼ linear interploationí•˜ì—¬ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³ , ë‘ weight ê³µê°„ ì‚¬ì´ì˜ interpolated netë“¤ì˜ ì„±ëŠ¥ì„ í™•ì¸í•œë‹¤.

<p align="center">
  <img src="https://github.com/user-attachments/assets/735228f3-3f5d-4aad-b498-df9be0a6cb21">
</p>
ìœ„ì˜ ê·¸ë¦¼ì—ì„œ (ìœ„: epoch 0)ì¸ ê²½ìš° errorê°€ í¬ê²Œ ë‚˜ì˜´ì„ í™•ì¸í•  ìˆ˜ìˆë‹¤. (ì•„ë˜ : epoch k)ì˜ ê²½ìš° kê°€ ì¦ê°€í• ìˆ˜ë¡ instabilityëŠ” ì ì  ê°ì†Œí•¨ì„ í™•ì¸í•  ìˆ˜ ìˆê³ , kê°€ ì ë‹¹íˆ ì‘ì€ ì§€ì ë¶€í„° lossê°€ ìƒë‹¹íˆ ë§ì´ ê°ì†Œí•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ê²°êµ­ **"íŠ¹ì • ì‹œì ë¶€í„° ë„¤íŠ¸ì›Œí¬ê°€ ìˆ˜ë ´í•˜ëŠ” ê³µê°„ì€ í•œ Flat ìœ„ì— ìˆì§€ ì•Šì€ê°€"** ë¼ëŠ” í•´ì„ì„ ì œì‹œí•œë‹¤.

### Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask [CV]
LTHëŠ” final weightì˜ L1 normìœ¼ë¡œ maskingì„ ìˆ˜í–‰í•œë‹¤.<br>
í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” inintial weightì™€ final weightì„ í•¨ê»˜ ê³ ë ¤í•œë‹¤. (NLPì˜ moving averageì™€ ìœ ì‚¬) <br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/2f50bcb2-4f01-4967-8b57-15ab53195394">
</p>

ìœ„ì˜ ì´ˆê¸° ê°€ì¤‘ì¹˜, ë§ˆì§€ë§‰ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œí•œ ê·¸ë˜í”„ë¥¼ í†µí•´ ë‹¤ì–‘í•œ í•´ì„ì„ í•  ìˆ˜ ìˆë‹¤.<br>
ì˜ˆë¥¼ ë“¤ì–´ "movement"ëŠ” ë³€ìœ„ë¥¼ ë³¸ê²ƒì´ë¼ í•´ì„í•  ìˆ˜ ìˆë‹¤.

### Movement Pruning: Adaptive Sparsity by Fine-Tuning [NLP]
Fine Tuning ê³¼ì •ì—ì„œ weightì˜ ì ˆëŒ€ì ì¸ í¬ê¸° ëŒ€ì‹ (Magnitude)ì— 0ì—ì„œ ë©€ì–´ì§€ëŠ”ì§€ ë˜ëŠ” ê°€ê¹Œì›Œì§€ëŠ”ì§€ë¥¼ ë³´ë©´ì„œ Pruningì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì´ë‹¤.

### Score-Based Pruning
Unstructured pruningì˜ ëŒ€í‘œì ì¸ ë°©ë²•ì´ë‹¤.

Score-Based Pruningì€ weightì™€ ë™ì¼í•œ í¬ê¸°ì˜ score í–‰ë ¬ì„ ê¸°ì¤€ìœ¼ë¡œ v%ëŠ” 1ë¡œ masking ê·¸ ì™¸ëŠ” 0ìœ¼ë¡œ maskingì„ í•¨ìœ¼ë¡œì¨ pruningì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.

ì´ë•Œ ScoreëŠ” Weightê°€ fine tuning ë˜ë©´ì„œ í•¨ê»˜ í•™ìŠµë˜ë¯€ë¡œ self correctionë˜ëŠ” íš¨ê³¼ê°€ ì¡´ì¬í•œë‹¤.


# Structured - UnStructured
- Structured Pruning
    - ì¥ì : ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ, ì†ë„ í–¥ìƒ
    - ë‹¨ì : ì„±ëŠ¥ ê°ì†Œ
- UnStructured Pruning
    - ì ì : ëª¨ë¸ ì‚¬ì´ì¦ˆ ê°ì†Œ, ì ì€ ì„±ëŠ¥ ê°ì†Œ
    - ë‹¨ì : ì†ë„ í–¥ìƒ ì—†ìŒ (í•˜ë“œì›¨ì–´)

# Pruning at initialization
ì¶”ê°€ë¡œ Pruning ì‹œì ì„ í•™ìŠµ ì´ì „ì— ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ë„ ì¡´ì¬í•œë‹¤.<br>
ì´ëŠ” í•™ìŠµ ì´ì „ì— "ì¤‘ìš”ë„"ë¥¼ ì¸¡ì •í•˜ì—¬ ìˆ˜í–‰í•˜ë©°, ì‹œê°„ ì ˆì•½ì˜ ì¥ì ì´ ì¡´ì¬í•œë‹¤.

ì´ë•Œ ë‹¤ìŒê³¼ ê°™ì€ **ì¤‘ìš”ë„ ê³„ì‚° ë°©ë²•**ì´ ì¡´ì¬í•œë‹¤.

- SNIP: Training ë°ì´í„° ìƒ˜í”Œ, Forwardí•´ì„œ Gredientì™€ Weightì˜ ê³±ì˜ ì ˆëŒ€ê°’
- GraSP : Training ë°ì´í„° ìƒ˜í”Œ, Forwardí•´ì„œ Hessian-gredient productì™€ Weightì˜ ê³±
- SynFlow : ì „ë¶€ 1ë¡œ ëœ ê°€ìƒ ë°ì´í„°ë¥¼ Forwardí•´ì„œ Gradientì™€ Weightì„ ê³±

### Zero-cost proxies for Lightweight NAS
í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” NASë¥¼ í†µí•´ ìœ„ì˜ ì¤‘ìš”ë„ ê³„ì‚°ë°©ë²•ì„ ì‹¤í—˜í•œë‹¤. <br>
ì‹¤í—˜ì„ í†µí•´ ì¤‘ìš”ë„ ì¸¡ì • scoreì™€ í•™ìŠµ ê²°ê³¼ê°„ì˜ ìƒê´€ ê³„ìˆ˜(Spearman ìƒê´€ê³„ìˆ˜)ê°€ ë†’ìŒì„ í™•ì¸í–ˆê³ , Synflowê°€ ë‹¤ì–‘í•œ taskì—ë„ ë†’ì€ ìƒê´€ ê³„ìˆ˜ë¥¼ ë³´ì˜€ë‹¤.

Pruning at initializationì€ í•™ìŠµì´ í•„ìš” ì—†ëŠ”, ê°„ì ‘ì ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œì¨ í™œìš©ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  í•œë‹¤.

# ğŸ”— Reference
[Paper]<br>
*Learning Efficient Convolutional Networks through Network Slimming<br>
HRank: Filter Pruning using High-Rank Feature Map<br>
Are Sixteen Heads Really Better than One?<br>
The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks<br>
Stabilizing the Lottery Ticket Hypothesis; Weight Rewinding<br>
Comparing Rewinding And Find-tuning In Neural Network Pruning; Learning Rate Rewinding<br>
Linear Mode Connectivity and the Lottery Ticket Hypothesis<br>
Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask<br>
Movement Pruning: Adaptive Sparsity by Fine-Tuning<br>
Zero-cost proxies for Lightweight NAS*
