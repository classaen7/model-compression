{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation for Quantization\n",
    "\n",
    "Torch에서는 Quantization이 Module by Module basis로 동작하기 때문에 아래와 같은 부수적인 것들은 준비해야 된다.\n",
    "\n",
    "1. Output requantization이 필요한 연산들에 대해서 funcational에서 `nn.module` 형태로 변환\n",
    "2. 어느 특정 레이어에 대해서 Quantized를 하려면 `.qconfig` 값을 별도로 세팅\n",
    "\n",
    "또한 아래의 PTQ, QAT와 같은 static quantization을 적용하려면 추가적인 준비가 더 필요하다.\n",
    "1. `QuantStub`, `DeQuantStub`이 모듈의 앞 뒤에 붙어야 함\n",
    "2. Quantization을 위한 특수한 핸들링(add or cat)을 수행할 때 `FloatFunctional` 사용이 요구 됨\n",
    "3. Layer Fusion을 위한 `Fuse modules` 사용\n",
    "\n",
    "먼저 Quantization이 가능한 단순한 floating point model을 정의하면 다음과 같다.<br>\n",
    "Quant, De-Quant가 layer를 감싸는 형식으로 구성해야하며, 가끔 연산이 수행되지 않는 layer의 경우 해당 layer를 제외하고 2개의 part로 quant-dequant로 묶어야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.quant = torch.quantization.QuantStub() # floating point tensor를 quantized tensor로 변환\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(3, 3, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "        self.linear = torch.nn.Linear(3 * 32 * 32, 10)\n",
    "\n",
    "        self.dequant = torch.quantization.DeQuantStub() #quantized tensor에서 floating point로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x) #모델의 앞에 Quant 추가 \n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dequant(x) #모델의 뒤에 De-Quant 추가\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization Approach는 크게 2가지로 나뉜다.\n",
    "\n",
    "# 1. PTQ : Post Training Quantization\n",
    "\n",
    "PTQ는 학습 후에 quantization parameter (scale, shift)를 결정한다.\n",
    "\n",
    "- **Dynamic range quantization**(weight only quantization)\n",
    "\n",
    "weight만 (일반적으로)8-bit로 quantize된다. 따라서 모델 용량은 1/4 정도 감소한다. 또한 별도의 calibration 데이터가 필요하지 않다.<br>\n",
    "inference 시에는 floating-point로변환되어 수행되므로 CPU 상의 속도 향상은 미미하다.\n",
    "\n",
    "\n",
    "- **Full integer quantization**(weight and activation quantization)\n",
    "\n",
    "weight 뿐만 아니라 모델의 입력 데이터, activations(중간 레이어의 output)들 또한 8-bit로 quantize<br>\n",
    "더 적은 메모리 사용량, Cache 재사용성 증가라는 장점이 있다.<br>\n",
    "하지만 Activations의 parameter를 결정하기 위한 calibration 데이터가 필요하다.<br>\n",
    "\n",
    "\n",
    "- TensorRT의 calibration<br>\n",
    "\n",
    "TensorRT는 zero point를 사용하지않는 Symmetric Quantization을 수행한다.<br>\n",
    "이때 calibration은 성능 저하를 최소로하는 `threshold` 및 `scale`을 찾게된다. <br>\n",
    "각 레이어 마다 activation value의 범위와 분포는 모두 다르다.<br>\n",
    "이때 특정 threshold에서 saturated된 normalized histogram 분포(ref_distr(P))와\n",
    "원 histogram로부터 quantized, normalized된 분포(quant_distr(Q))의\n",
    "KL divergence를 측정하고 최소인 지점을 threshold로 설정한다.<br>\n",
    "\n",
    "\n",
    "- **Float16 quantization**\n",
    "\n",
    "fp32의 데이터 타입의 weight를 fp16으로 quantize<br>\n",
    "모델 용량이 1/2 줄어들고 성능 저하가 적다. 또한 GPU 상에서 빠른 연산이 가능하다.<br>\n",
    "하지만 반대로 CPU 상에서는 fixed point 연산만큼의 속도 향상이 있지는 않다.<br>\n",
    "\n",
    "다음과 같이 Inference시에 `half()`를 통해서 입력 데이터와 모델의 dtype을 fp16으로 간단히 변환할 수 있다. <br>\n",
    "학습 과정에서는 Mixed precision 기능을 활용하면된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(model, testloader, device, half=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model = model.to(device)\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if half:\n",
    "            images = images.half()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, axis=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "\n",
    "model = load_model(\"fp32_path\")\n",
    "model.eval()\n",
    "fp16_model = model.half()\n",
    "test_model(model=fp16_model,\n",
    "           testloader=testloader,\n",
    "           device=device,\n",
    "           half=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ 적용 과정\n",
    "\n",
    "1. Quantization configuration<br>\n",
    ": 어떤 하드웨어를 사용하냐에 따라서 다른 backend값 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"fbgemm\"\n",
    "model_fp32.qconfig = torch.quantization.get_default_config(backend)​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Layer Fusion 수행: Fuse & Prepare<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calibration 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration with Train Data\n",
    "test_model(model_fp32_prepared, trainloader, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Convert 수행 (int8 형태로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = torch.quantization.convert(model_fp32_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. QTA : Quantization Aware Training\n",
    "\n",
    "QAT는 학습 시점에 quantization을 emulate 하여 , 추론 시에 발생하는 quantization 오류를 학습 시점에 반영가능하도록 하는 방법<br>\n",
    "PTQ 대비 quantization으로 인한 성능 하락 폭이 적은 것이 큰 장점이다.\n",
    "\n",
    "학습에서 back propagation을 적용하기 위해 floating point로 변하게 된다. 이때 y=x 모양의 형태로 변해서 gradient를 linear(=1)로 가정하여 네트워크 학습을 수행한다.<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAT 적용 과정\n",
    "\n",
    "PTQ 적용하는 과정이랑 거의 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하드웨어에 따른 backend 설정\n",
    "backend = \"fbgemm\"\n",
    "model_fp32.qconfig = torch.quantization.get_default_config(backend)\n",
    "\n",
    "# layer fusion & prepare\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "# calibration\n",
    "train(model_fp32_prepared)\n",
    "\n",
    "# conver into int-8\n",
    "model_int8_qat = torch.quantization.convert(model_fp32_prepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlpoetry-5eG4FEyD-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
